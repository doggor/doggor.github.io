<!doctype html><html lang=zh-TW><head><title>使用 PyTorch 和 BertForMaskedLM 偵測錯別字並建議正確用字 // 程式魔法陣</title>
<meta charset=utf-8><meta name=generator content="Hugo 0.126.2"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Doggor"><meta name=description content="在自然語言處理中，偵測錯別字並提供正確的修正建議是一個重要的任務。本文將介紹如何使用 PyTorch 和 BertForMaskedLM 模型來實現這一功能。我們將使用 BERT 模型進行預訓練，並利用其能力來預測遺漏或錯誤的詞彙。"><link rel=manifest href=/manifest.json><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="black"><meta name=apple-mobile-web-app-title content="程式魔法陣"><link rel=apple-touch-icon href=/images/icons/icon-152x152.png><meta name=theme-color content="#16191d"><meta property="og:image" content="https://doggor.github.io//posts/2024/bert-typo-detection/thumbnail.jpg"><meta property="og:tags" content="pytorch"><meta property="og:tags" content="bert"><meta property="og:tags" content="typo-detection"><base href=https://doggor.github.io/><style>body{background-color:#16191d;color:#16191d}a{color:#16191d}input,img{display:none}</style><link rel=preload as=style href=https://doggor.github.io/css/main.min.1ef0225360632f59312aae692e198f69.css onload='this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://doggor.github.io/css/main.min.1ef0225360632f59312aae692e198f69.css></noscript><link rel=preload as=style media="screen and (max-width: 939px)" href=https://doggor.github.io/css/mobile.min.59580f7f6c75a063777b2241648dadac.css onload='this.rel="stylesheet"'><noscript><link rel=stylesheet media="screen and (max-width: 939px)" href=https://doggor.github.io/css/mobile.min.59580f7f6c75a063777b2241648dadac.css></noscript><script src=https://doggor.github.io/js/bundle.min.c765417f5f9a85fd757fd8638bf5aa1a.js async></script></head><body class=darkmode--activated><div class=app-centralizer><header class=app-header><div class=mobile-back-btn></div><h3 class=app-header-wanna-say>&nbsp;</h3><h1 class=app-header-my-face>_(:з」∠)_</h1><a class=app-header-title href=/ aria-label=Home><h2>#程式魔法陣</h2></a><div class=app-header-tag-group><a class=tag-btn href=/tags/bert/ aria-label=tag:bert><span>bert</span>
</a><a class=tag-btn href=/tags/binlog/ aria-label=tag:binlog><span>binlog</span>
</a><a class=tag-btn href=/tags/canvas/ aria-label=tag:canvas><span>canvas</span>
</a><a class=tag-btn href=/tags/centos/ aria-label=tag:centos><span>centos</span>
</a><a class=tag-btn href=/tags/cert/ aria-label=tag:cert><span>cert</span>
</a><a class=tag-btn href=/tags/ci/ aria-label=tag:ci><span>ci</span>
</a><a class=tag-btn href=/tags/crawler/ aria-label=tag:crawler><span>crawler</span>
</a><a class=tag-btn href=/tags/data/ aria-label=tag:data><span>data</span>
</a><a class=tag-btn href=/tags/debug/ aria-label=tag:debug><span>debug</span>
</a><a class=tag-btn href=/tags/design-pattern/ aria-label=tag:design-pattern><span>design-pattern</span>
</a><a class=tag-btn href=/tags/docker/ aria-label=tag:docker><span>docker</span>
</a><a class=tag-btn href=/tags/exam/ aria-label=tag:exam><span>exam</span>
</a><a class=tag-btn href=/tags/fail2ban/ aria-label=tag:fail2ban><span>fail2ban</span>
</a><a class=tag-btn href=/tags/firewall/ aria-label=tag:firewall><span>firewall</span>
</a><a class=tag-btn href=/tags/ftp/ aria-label=tag:ftp><span>ftp</span>
</a><a class=tag-btn href=/tags/git/ aria-label=tag:git><span>git</span>
</a><a class=tag-btn href=/tags/gitlab/ aria-label=tag:gitlab><span>gitlab</span>
</a><a class=tag-btn href=/tags/google/ aria-label=tag:google><span>google</span>
</a><a class=tag-btn href=/tags/gzip/ aria-label=tag:gzip><span>gzip</span>
</a><a class=tag-btn href=/tags/headless-chrome/ aria-label=tag:headless-chrome><span>headless-chrome</span>
</a><a class=tag-btn href=/tags/html/ aria-label=tag:html><span>html</span>
</a><a class=tag-btn href=/tags/indexing/ aria-label=tag:indexing><span>indexing</span>
</a><a class=tag-btn href=/tags/influx/ aria-label=tag:influx><span>influx</span>
</a><a class=tag-btn href=/tags/js/ aria-label=tag:js><span>js</span>
</a><a class=tag-btn href=/tags/learning-notes/ aria-label=tag:learning-notes><span>learning-notes</span>
</a><a class=tag-btn href=/tags/malicious/ aria-label=tag:malicious><span>malicious</span>
</a><a class=tag-btn href=/tags/marketing/ aria-label=tag:marketing><span>marketing</span>
</a><a class=tag-btn href=/tags/mysql/ aria-label=tag:mysql><span>mysql</span>
</a><a class=tag-btn href=/tags/nginx/ aria-label=tag:nginx><span>nginx</span>
</a><a class=tag-btn href=/tags/nodejs/ aria-label=tag:nodejs><span>nodejs</span>
</a><a class=tag-btn href=/tags/ntp/ aria-label=tag:ntp><span>ntp</span>
</a><a class=tag-btn href=/tags/php/ aria-label=tag:php><span>php</span>
</a><a class=tag-btn href=/tags/puppeteer/ aria-label=tag:puppeteer><span>puppeteer</span>
</a><a class=tag-btn href=/tags/pytorch/ aria-label=tag:pytorch><span>pytorch</span>
</a><a class=tag-btn href=/tags/react/ aria-label=tag:react><span>react</span>
</a><a class=tag-btn href=/tags/repair/ aria-label=tag:repair><span>repair</span>
</a><a class=tag-btn href=/tags/rust/ aria-label=tag:rust><span>rust</span>
</a><a class=tag-btn href=/tags/search-ads/ aria-label=tag:search-ads><span>search-ads</span>
</a><a class=tag-btn href=/tags/seo/ aria-label=tag:seo><span>seo</span>
</a><a class=tag-btn href=/tags/tensorflow/ aria-label=tag:tensorflow><span>tensorflow</span>
</a><a class=tag-btn href=/tags/tool/ aria-label=tag:tool><span>tool</span>
</a><a class=tag-btn href=/tags/typo-detection/ aria-label=tag:typo-detection><span>typo-detection</span>
</a><a class=tag-btn href=/tags/vscode/ aria-label=tag:vscode><span>vscode</span></a></div><div class=app-header-links><a class=tag-link href=/categories aria-label=Categories><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-folder"><path d="M22 19a2 2 0 01-2 2H4a2 2 0 01-2-2V5a2 2 0 012-2h5l2 3h9a2 2 0 012 2z"/></svg></a>
<a class=tag-link href=/tags aria-label=Tags><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tag"><path d="M20.59 13.41l-7.17 7.17a2 2 0 01-2.83.0L2 12V2h10l8.59 8.59a2 2 0 010 2.82z"/><line x1="7" y1="7" x2="7" y2="7"/></svg></a>
<a target=_blank href=https://github.com/doggor rel=noopener aria-label=Github><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-github"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg></a>
<a target=_blank href=https://www.linkedin.com/in/andy-ching-hk rel=noopener aria-label=Github><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-linkedin"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6z"/><rect x="2" y="9" width="4" height="12"/><circle cx="4" cy="4" r="2"/></svg></a></div><label class=planet-switch><input type=checkbox name=darkmode checked aria-label="toggle darkmode" id=darkmode-switch-checkbox><div class="planet moon"><div class=rays><div class=ray></div><div class=ray></div><div class=ray></div></div><div class=core></div><div class=hole></div><div class=hole></div></div></label></header><div class=app-topbar><div class=app-topbar-menu-btn aria-label=menu></div><a href=/ aria-label=Home><h3>#程式魔法陣</h3></a></div><main class=app-container><article class=post><header class=post-header><div><picture><source srcset="/posts/2024/bert-typo-detection/thumbnail_hu5459c0360c2b0cb7a147d2df0eb350ca_635271_1080x140_fill_q75_h2_box_center.webp, /posts/2024/bert-typo-detection/thumbnail_hu5459c0360c2b0cb7a147d2df0eb350ca_635271_1920x280_fill_q75_h2_box_center.webp 2x" type=image/webp><source srcset="/posts/2024/bert-typo-detection/thumbnail_hu5459c0360c2b0cb7a147d2df0eb350ca_635271_1080x140_fill_q75_box_center.jpg, /posts/2024/bert-typo-detection/thumbnail_hu5459c0360c2b0cb7a147d2df0eb350ca_635271_1920x280_fill_q75_box_center.jpg 2x" type=image/jpg><img class=post-cover src=/posts/2024/bert-typo-detection/thumbnail_hu5459c0360c2b0cb7a147d2df0eb350ca_635271_1080x140_fill_q75_box_center.jpg alt></picture></div><div><h1 class=post-title>使用 PyTorch 和 BertForMaskedLM 偵測錯別字並建議正確用字</h1></div><div class=post-meta><div><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>
Jun 1, 2024</div><div><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>
5 min read</div><div><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tag"><path d="M20.59 13.41l-7.17 7.17a2 2 0 01-2.83.0L2 12V2h10l8.59 8.59a2 2 0 010 2.82z"/><line x1="7" y1="7" x2="7" y2="7"/></svg>
<a class=tag href=https://doggor.github.io/tags/pytorch/>pytorch</a><a class=tag href=https://doggor.github.io/tags/bert/>bert</a><a class=tag href=https://doggor.github.io/tags/typo-detection/>typo-detection</a></div></div></header><div class=post-content><p>在自然語言處理中，偵測錯別字並提供正確的修正建議是一個重要的任務。本文將介紹如何使用 PyTorch 和 BertForMaskedLM 模型來實現這一功能。我們將使用 BERT 模型進行預訓練，並利用其能力來預測遺漏或錯誤的詞彙。</p><h1 id=1-簡介>1. 簡介</h1><p>在這個實現中，我們將使用 Hugging Face 的 transformers 庫，這是一個廣泛使用的自然語言處理庫，提供了各種預訓練模型和相關工具。</p><p>我們將使用 BERT 模型的中文預訓練版本 &ldquo;bert-base-chinese&rdquo; 來進行錯別字偵測和修正建議。我們將自定義一個繼承自 BertForMaskedLM 的模型，並在 forward 方法中實現主要的邏輯。</p><h1 id=2-實現步驟>2. 實現步驟</h1><p>以下是我們實現錯別字檢測功能的步驟：</p><ol><li><strong>模型建立：</strong> 我們創建一個自定義的 BERT 模型，繼承自 BertForMaskedLM。這個模型的目的是預測每個 token 的概率分佈，用以判斷錯別字（概率較低）及取得建議用字（概率較高）。</li><li><strong>資料預處理：</strong> 我們使用 BERT 預訓練模型的 tokenizer 將句子轉換為 token IDs。同時，我們在句子前添加 [UNK] 作為特殊標記，以避免首字被錯誤地判定為錯別字。</li><li><strong>錯別字檢測：</strong> 我們遍歷每個 token，比較原始 token 和預測 token 是否相同。如果不同，我們將其視為錯別字。我們還計算了每個 token 的概率，以便後續建議正確用字。</li><li><strong>結果呈現：</strong> 我們將錯別字的位置、原始字符、預測字符和其概率列出，以便用戶查看。</li></ol><h2 id=21-庫安裝>2.1. 庫安裝</h2><p>首先，確保你已經安裝了 transformers 庫。可以使用以下命令進行安裝：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>pip install transformers
</span></span></code></pre></div><h2 id=22-導入庫和設置參數>2.2 導入庫和設置參數</h2><p>我們首先導入所需的庫，並設置一些參數。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> AutoTokenizer, BertConfig, BertForMaskedLM
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torch <span style=color:#f92672>import</span> nn, topk, multiply
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>seq_length <span style=color:#f92672>=</span> <span style=color:#ae81ff>128</span>
</span></span><span style=display:flex><span>pretrained_model_name <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;bert-base-chinese&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(pretrained_model_name)
</span></span><span style=display:flex><span>vocab_size <span style=color:#f92672>=</span> tokenizer<span style=color:#f92672>.</span>vocab_size
</span></span></code></pre></div><p>在這裡，我們使用 AutoTokenizer 從預訓練模型中創建一個 tokenizer。 <code>vocab_size</code> 表示模型的詞彙總數。句子的最大可接受長度 <code>seq_length</code> 是 128 個 token（詞彙）。</p><h2 id=23-自定義模型>2.3. 自定義模型</h2><p>接下來，我們自定義一個繼承自 BertForMaskedLM 的模型，並重寫其中的 forward 方法。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>MyModel</span>(BertForMaskedLM):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, input_ids, attention_mask, token_type_ids):
</span></span><span style=display:flex><span>        attentions <span style=color:#f92672>=</span> super()<span style=color:#f92672>.</span>forward(input_ids<span style=color:#f92672>=</span>input_ids, attention_mask<span style=color:#f92672>=</span>attention_mask, token_type_ids<span style=color:#f92672>=</span>token_type_ids, labels<span style=color:#f92672>=</span>input_ids)
</span></span><span style=display:flex><span>        probs <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Softmax(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)(attentions<span style=color:#f92672>.</span>logits)
</span></span><span style=display:flex><span>        predict_probs, predict_ids <span style=color:#f92672>=</span> topk(probs, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        input_probs, _ <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>max(multiply(probs, nn<span style=color:#f92672>.</span>functional<span style=color:#f92672>.</span>one_hot(input_ids, vocab_size)), <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> input_ids, input_probs, predict_ids, predict_probs
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> MyModel<span style=color:#f92672>.</span>from_pretrained(pretrained_model_name)
</span></span></code></pre></div><p>在這個自定義模型中，我們調用了 BertForMaskedLM 的 <code>forward</code> 方法，並通過計算輸入序列的注意力、預測概率和預測詞彙等來擴展其功能。以下是這個模型的 <code>forward</code> 方法各個步驟的解釋：</p><ol><li><strong>BertForMaskedLM 的 forward 方法：</strong> 在 MyModel 的 <code>forward</code> 方法中，首先調用了 BertForMaskedLM 的 <code>forward</code> 方法，傳遞了 <code>input_ids</code>、<code>attention_mask</code>、<code>token_type_ids</code> 和 <code>labels</code>。這將觸發 BERT 模型的正向傳遞運算，該運算會生成預測的詞彙概率。</li><li><strong>Softmax 轉換：</strong> 獲得 BERT 模型的輸出後，將詞彙概率 <code>attentions.logits</code> 通過 Softmax 函數進行轉換，以獲得正規化的概率分佈。這一步將確保每個詞彙的概率值在 0 到 1 之間，並且總和為 1。</li><li><strong>預測概率和預測詞彙的選取：</strong> 使用 <code>topk</code> 函數，從正規化的概率分佈中選取概率最高的詞彙。在這個例子中，選取了概率最高的一個詞彙作為預測的詞彙，並獲得對應的概率值。</li><li><strong>輸入詞彙的概率計算：</strong> 通過使用 <code>nn.functional.one_hot</code> 將 <code>input_ids</code> 轉換為 one-hot 表示，然後將之與正規化的概率分佈進行乘法運算，得到輸入詞彙在概率分佈中的相應概率值。這一步的目的是獲取模型對於輸入詞彙的預測概率。</li><li><strong>返回結果：</strong> <code>forward</code> 方法返回四個值：<code>input_ids</code> 是原始的輸入詞彙 ID 序列，<code>input_probs</code> 是模型對於輸入詞彙的預測概率，<code>predict_ids</code> 是預測的詞彙 ID，<code>predict_probs</code> 是預測詞彙的概率值。</li></ol><h2 id=24-偵測錯別字並提供建議>2.4. 偵測錯別字並提供建議</h2><p>現在，我們可以定義一個函數 <code>detect_typo</code> 來偵測錯別字並提供修正建議。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>detect_typo</span>(strings):
</span></span><span style=display:flex><span>    prefix <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;[UNK]&#34;</span>
</span></span><span style=display:flex><span>    input_strings <span style=color:#f92672>=</span> [ <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{</span>prefix<span style=color:#e6db74>}{</span>sentence<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span> <span style=color:#66d9ef>for</span> sentence <span style=color:#f92672>in</span> strings]
</span></span><span style=display:flex><span>    batch_encoding <span style=color:#f92672>=</span> tokenizer(input_strings, padding<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;max_length&#39;</span>, max_length<span style=color:#f92672>=</span>seq_length, return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;pt&#34;</span>)
</span></span><span style=display:flex><span>    input_ids, input_probs, predict_ids, predict_probs <span style=color:#f92672>=</span> model(<span style=color:#f92672>**</span>batch_encoding)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    results <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> batch_idx, input_tokens <span style=color:#f92672>in</span> enumerate(input_ids):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> token_idx, input_token <span style=color:#f92672>in</span> enumerate(input_tokens):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> input_token <span style=color:#f92672>==</span> tokenizer<span style=color:#f92672>.</span>sep_token_id: <span style=color:#66d9ef>break</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> input_token <span style=color:#f92672>==</span> tokenizer<span style=color:#f92672>.</span>cls_token_id: <span style=color:#66d9ef>continue</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> input_token <span style=color:#f92672>==</span> tokenizer<span style=color:#f92672>.</span>unk_token_id: <span style=color:#66d9ef>continue</span>
</span></span><span style=display:flex><span>            predict_token <span style=color:#f92672>=</span> predict_ids[batch_idx][token_idx]
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> predict_token <span style=color:#f92672>in</span> tokenizer<span style=color:#f92672>.</span>all_special_ids: <span style=color:#66d9ef>continue</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> input_token <span style=color:#f92672>!=</span> predict_token:
</span></span><span style=display:flex><span>                input_prob   <span style=color:#f92672>=</span> input_probs[batch_idx][token_idx]<span style=color:#f92672>.</span>item()
</span></span><span style=display:flex><span>                predict_prob <span style=color:#f92672>=</span> predict_probs[batch_idx][token_idx]<span style=color:#f92672>.</span>item()
</span></span><span style=display:flex><span>                input_char   <span style=color:#f92672>=</span> tokenizer<span style=color:#f92672>.</span>decode(input_token)
</span></span><span style=display:flex><span>                predict_char <span style=color:#f92672>=</span> tokenizer<span style=color:#f92672>.</span>decode(predict_token)
</span></span><span style=display:flex><span>                token_start, token_end <span style=color:#f92672>=</span> batch_encoding<span style=color:#f92672>.</span>token_to_chars(batch_idx, token_idx)
</span></span><span style=display:flex><span>                token_start <span style=color:#f92672>=</span> token_start <span style=color:#f92672>-</span> len(prefix)
</span></span><span style=display:flex><span>                token_end <span style=color:#f92672>=</span> token_end <span style=color:#f92672>-</span> len(prefix)
</span></span><span style=display:flex><span>                results<span style=color:#f92672>.</span>append({
</span></span><span style=display:flex><span>                    <span style=color:#e6db74>&#34;start&#34;</span>: token_start,
</span></span><span style=display:flex><span>                    <span style=color:#e6db74>&#34;end&#34;</span>: token_end,
</span></span><span style=display:flex><span>                    <span style=color:#e6db74>&#34;origin&#34;</span>: input_char,
</span></span><span style=display:flex><span>                    <span style=color:#e6db74>&#34;origin_prob&#34;</span>: input_prob,
</span></span><span style=display:flex><span>                    <span style=color:#e6db74>&#34;predict&#34;</span>: predict_char,
</span></span><span style=display:flex><span>                    <span style=color:#e6db74>&#34;predict_prob&#34;</span>: predict_prob,
</span></span><span style=display:flex><span>                })
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> results
</span></span></code></pre></div><p>在這個函數中，我們首先在每個句子前添加了 [UNK]，這是為了避免首字被錯誤地判定為錯別字並建議使用 &ldquo;。&rdquo; 的問題。然後，我們使用 tokenizer 將句子轉換為模型所需的輸入張量。接著，我們通過調用自定義模型的 <code>forward</code> 方法，獲取輸入序列的原始詞彙、原始詞彙概率、預測詞彙和預測詞彙概率。</p><blockquote><p>BertForMaskedLM 是一個基於 BERT 模型的語言模型，用於預測遮蔽（mask）的詞彙。當使用這個模型時，原句子的第一個字無論是甚麼，其概率通常會較低，而被預測為「。」的概率則較高。</p><p>這是因為在預訓練 BERT 時使用的訓練數據中，句子的開頭常常是以句點「。」或其他類似的標點符號結束。因此，模型在處理這種類型的句子時，經常遇到以句點結束的上下文。這種觀察在訓練過程中被模型學習到，導致模型傾向於預測句點作為開頭的可能性較高。</p><p>由於 BERT 是一種上下文敏感的模型，這種偏好可能在預測時產生影響，導致模型偏向選擇「。」作為開頭的預測結果。</p><p>需要注意的是，這種行為可能會因具體的 BERT 模型、訓練數據和任務特定的微調方式而有所不同。因此，具體的概率值可能會有所變化，但觀察到的趨勢通常是第一個字的概率較低，而「。」的概率較高。</p></blockquote><p>最後，我們遍歷輸入序列中的每個詞彙，檢查是否有錯別字。如果有，我們收集相關信息，如錯誤詞彙的位置、原始詞彙、原始詞彙概率、建議的正確詞彙以及建議詞彙的概率。</p><h1 id=3-執行錯別字偵測和修正>3. 執行錯別字偵測和修正</h1><p>現在，我們可以使用 <code>detect_typo</code> 函數來執行錯別字偵測和修正：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span>results <span style=color:#f92672>=</span> detect_typo([<span style=color:#e6db74>&#34;沒有情緒就不會被嘞索，沒有道德就不會被綁架。&#34;</span>])
</span></span><span style=display:flex><span>print(results)
</span></span></code></pre></div><p>在這個示例中，我們將一個句子作為輸入傳遞給 <code>detect_typo</code> 函數，並打印出偵測到的錯別字和修正建議。結果如下：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span>[{<span style=color:#e6db74>&#39;start&#39;</span>: <span style=color:#ae81ff>8</span>, <span style=color:#e6db74>&#39;end&#39;</span>: <span style=color:#ae81ff>9</span>, <span style=color:#e6db74>&#39;origin&#39;</span>: <span style=color:#e6db74>&#39;嘞&#39;</span>, <span style=color:#e6db74>&#39;origin_prob&#39;</span>: <span style=color:#ae81ff>1.4988561503059827e-08</span>, <span style=color:#e6db74>&#39;predict&#39;</span>: <span style=color:#e6db74>&#39;勒&#39;</span>, <span style=color:#e6db74>&#39;predict_prob&#39;</span>: <span style=color:#ae81ff>0.9008431434631348</span>}]
</span></span></code></pre></div><p>在這句話中，第9個字「嘞」被判定為錯別字。根據模型的預測，建議將該字替換為「勒」。原始詞彙「嘞」的概率非常低（約為 1.5e-08），而建議詞彙「勒」的概率則非常高（約為 0.901）。</p><p>這個結果顯示出在給定的句子中，模型能夠成功地偵測到錯別字並提供了合理的修正建議。</p><h1 id=4-結論>4. 結論</h1><p>本文介紹了如何使用 PyTorch 和 BertForMaskedLM 模型來實現錯別字偵測和修正建議的功能。通過使用預訓練的 BERT 模型，我們能夠利用其強大的語言建模能力來預測遺漏或錯誤的詞彙。這種方法對於語言處理應用中的錯別字修正和文字校對等任務非常有用。</p><p>希望本文能幫助你理解如何使用 PyTorch 和 BertForMaskedLM 來實現錯別字偵測和修正建議的功能。如果你有任何問題或建議，請隨時在下方留言。</p></div></article><div id=share-buttons><div class=facebook title="Share this on Facebook" onclick='window.open("http://www.facebook.com/share.php?u=https://doggor.github.io//posts/2024/bert-typo-detection/")'><svg viewBox="0 0 1792 1792" xmlns="http://www.w3.org/2000/svg"><path d="M1343 12v264h-157q-86 0-116 36t-30 108v189h293l-39 296h-254v759H734V905H479V609h255V391q0-186 104-288.5T1115 0q147 0 228 12z"/></svg></div><div class=twitter title="Share this on Twitter" onclick='window.open("https://twitter.com/intent/tweet?url=https://doggor.github.io//posts/2024/bert-typo-detection/&text=使用 PyTorch 和 BertForMaskedLM 偵測錯別字並建議正確用字")'><svg viewBox="0 0 1792 1792" xmlns="http://www.w3.org/2000/svg"><path d="M1684 408q-67 98-162 167 1 14 1 42 0 130-38 259.5T1369.5 1125 1185 1335.5t-258 146-323 54.5q-271 0-496-145 35 4 78 4 225 0 401-138-105-2-188-64.5T285 1033q33 5 61 5 43 0 85-11-112-23-185.5-111.5T172 710v-4q68 38 146 41-66-44-105-115t-39-154q0-88 44-163 121 149 294.5 238.5T884 653q-8-38-8-74 0-134 94.5-228.5T1199 256q140 0 236 102 109-21 205-78-37 115-142 178 93-10 186-50z"/></svg></div><div class=linkedin title="Share this on Linkedin" onclick='window.open("https://www.linkedin.com/shareArticle?mini=true&url=https://doggor.github.io//posts/2024/bert-typo-detection/&title=使用 PyTorch 和 BertForMaskedLM 偵測錯別字並建議正確用字&summary=<p>在自然語言處理中，偵測錯別字並提供正確的修正建議是一個重要的任務。本文將介紹如何使用 PyTorch 和 BertForMaskedLM 模型來實現這一功能。我們將使用 BERT 模型進行預訓練，並利用其能力來預測遺漏或錯誤的詞彙。</p>&source=https://doggor.github.io/")'><svg viewBox="0 0 1792 1792" xmlns="http://www.w3.org/2000/svg"><path d="M477 625v991H147V625h330zm21-306q1 73-50.5 122T312 490h-2q-82 0-132-49t-50-122q0-74 51.5-122.5T314 148t133 48.5T498 319zm1166 729v568h-329v-530q0-105-40.5-164.5T1168 862q-63 0-105.5 34.5T999 982q-11 30-11 81v553H659q2-399 2-647t-1-296l-1-48h329v144h-2q20-32 41-56t56.5-52 87-43.5T1285 602q171 0 275 113.5t104 332.5z"/></svg></div><div class=mail title="Share this through Email" onclick='window.open("mailto:?&&subject=使用 PyTorch 和 BertForMaskedLM 偵測錯別字並建議正確用字&body=<p>在自然語言處理中，偵測錯別字並提供正確的修正建議是一個重要的任務。本文將介紹如何使用 PyTorch 和 BertForMaskedLM 模型來實現這一功能。我們將使用 BERT 模型進行預訓練，並利用其能力來預測遺漏或錯誤的詞彙。</p>%0A%0Ahttps://doggor.github.io//posts/2024/bert-typo-detection/")'><svg viewBox="0 0 1792 1792" xmlns="http://www.w3.org/2000/svg"><path d="M1792 710v794q0 66-47 113t-113 47H160q-66 0-113-47T0 1504V710q44 49 101 87 362 246 497 345 57 42 92.5 65.5t94.5 48 110 24.5h2q51 0 110-24.5t94.5-48 92.5-65.5q170-123 498-345 57-39 1e2-87zm0-294q0 79-49 151t-122 123q-376 261-468 325-10 7-42.5 30.5t-54 38-52 32.5-57.5 27-50 9h-2q-23 0-50-9t-57.5-27-52-32.5-54-38T639 1015q-91-64-262-182.5T172 690q-62-42-117-115.5T0 438q0-78 41.5-130T160 256h1472q65 0 112.5 47t47.5 113z"/></svg></div></div><section class=disqus-area><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//doggor-coding-notebook.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></section></main></div></body></html>